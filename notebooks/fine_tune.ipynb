{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVyXtoEPDtqzFoX6qgiUto"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c165d74fc80040aebf91a607919e06ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_961a1c526c2e4899a60a7b1e9ed3b453","IPY_MODEL_7d5f0a749f794854b47c9552e7ca6c9c","IPY_MODEL_2e884283b3fb48fb8f4dbdb3aea709f1"],"layout":"IPY_MODEL_c9fd7ae3624b4c4e88083c2c7e7973e4"}},"961a1c526c2e4899a60a7b1e9ed3b453":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5bea9323d4a4e2a865761f3a727edd4","placeholder":"‚Äã","style":"IPY_MODEL_4752e8b753b74bb4bd0232e96772da25","value":"Map:‚Äá100%"}},"7d5f0a749f794854b47c9552e7ca6c9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_071932f7ee6143789e724b2b618d386f","max":8075,"min":0,"orientation":"horizontal","style":"IPY_MODEL_303574550d3b450297528aaa4651a6be","value":8075}},"2e884283b3fb48fb8f4dbdb3aea709f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e048373ced1447f8e0e0809b2eedb50","placeholder":"‚Äã","style":"IPY_MODEL_c87106495311442c9a95b4d23f990695","value":"‚Äá8075/8075‚Äá[00:01&lt;00:00,‚Äá9031.55‚Äáexamples/s]"}},"c9fd7ae3624b4c4e88083c2c7e7973e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5bea9323d4a4e2a865761f3a727edd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4752e8b753b74bb4bd0232e96772da25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"071932f7ee6143789e724b2b618d386f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"303574550d3b450297528aaa4651a6be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e048373ced1447f8e0e0809b2eedb50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c87106495311442c9a95b4d23f990695":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e784b89ff98f41ab9860f3a3efe3b556":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d846ac993f9e4513ace7d539e4460676","IPY_MODEL_15024f288f8043fcb19824502bdebe39","IPY_MODEL_cf729487f6424863a00e1c8f58ca7a68"],"layout":"IPY_MODEL_139bc3967a7b4b13903130291d27ea01"}},"d846ac993f9e4513ace7d539e4460676":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca4ed8a848d24359b23d28d83d952b34","placeholder":"‚Äã","style":"IPY_MODEL_4e47040475614f4daf9f016457a92c20","value":"Map:‚Äá100%"}},"15024f288f8043fcb19824502bdebe39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62a3dd6f7434401ab40c0737075649bc","max":6460,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c35d63ef6b347649e854e75a86e4cfc","value":6460}},"cf729487f6424863a00e1c8f58ca7a68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15cd7520006f483cb0007d2d7f7ab9c4","placeholder":"‚Äã","style":"IPY_MODEL_d6c08c84533a4669b7441091c648b447","value":"‚Äá6460/6460‚Äá[00:00&lt;00:00,‚Äá7199.50‚Äáexamples/s]"}},"139bc3967a7b4b13903130291d27ea01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca4ed8a848d24359b23d28d83d952b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e47040475614f4daf9f016457a92c20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62a3dd6f7434401ab40c0737075649bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c35d63ef6b347649e854e75a86e4cfc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15cd7520006f483cb0007d2d7f7ab9c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6c08c84533a4669b7441091c648b447":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d1bbe57d9b74ca5800d2def6a06758b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_367be2cf21a14684bb4c2c8c2b11036e","IPY_MODEL_3fa9e5ede949494690a50c83c7088470","IPY_MODEL_f67a0318de924f8e8cbe0e1c771cc833"],"layout":"IPY_MODEL_dc2a08c707a0425eb78481e2e2f22b79"}},"367be2cf21a14684bb4c2c8c2b11036e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_171a3dc5c72a42a1bc1fb717e152eaea","placeholder":"‚Äã","style":"IPY_MODEL_d1e70c3bc0ff45d9b5dc2a4f16db45c1","value":"Map:‚Äá100%"}},"3fa9e5ede949494690a50c83c7088470":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f3702cac459479c8f0cbfea2b1ba40b","max":1615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b44b4482ca3e4fc7a4380f49de828b41","value":1615}},"f67a0318de924f8e8cbe0e1c771cc833":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20d6c005f38747f498411526f19c66b4","placeholder":"‚Äã","style":"IPY_MODEL_095f184e73644fa5bb34b36d70aee69a","value":"‚Äá1615/1615‚Äá[00:00&lt;00:00,‚Äá6273.95‚Äáexamples/s]"}},"dc2a08c707a0425eb78481e2e2f22b79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"171a3dc5c72a42a1bc1fb717e152eaea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1e70c3bc0ff45d9b5dc2a4f16db45c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f3702cac459479c8f0cbfea2b1ba40b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b44b4482ca3e4fc7a4380f49de828b41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20d6c005f38747f498411526f19c66b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"095f184e73644fa5bb34b36d70aee69a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bfqQdi6eWt3j"},"outputs":[],"source":[]},{"cell_type":"code","source":["!pip install transformers datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-UPrWoBqW9pO","executionInfo":{"status":"ok","timestamp":1727780737411,"user_tz":-180,"elapsed":4359,"user":{"displayName":"Yonas Kumelachew","userId":"02986940558465790424"}},"outputId":"bdfd1ab5-af04-48d1-9cc0-174346276a64"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","\n","# Step 1: Load the data\n","df = pd.read_csv(\"/content/drive/My Drive/Kifiya/week 5/final_tokens_labels.csv\")\n","\n","# Display the first few rows to understand the structure\n","print(df.head())\n","\n","# Step 2: Process the 'Token' and 'Label' columns\n","df['Token'] = df['Token'].apply(lambda x: x.split())  # Split tokens\n","df['Label'] = df['Label'].apply(lambda x: x.split())  # Split labels\n","\n","# Step 3: Create a Hugging Face Dataset\n","dataset = Dataset.from_pandas(df)\n","\n","# Check the dataset structure\n","print(dataset)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0Jgt_n2tMFF","executionInfo":{"status":"ok","timestamp":1727783971033,"user_tz":-180,"elapsed":510,"user":{"displayName":"Yonas Kumelachew","userId":"02986940558465790424"}},"outputId":"a90728f9-3a1c-462d-ea02-cd0e2beed3f8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["     Token      Label\n","0  ·â£·àà·â•·ãô·å•·âÖ·àù  I-PRODUCT\n","1   ·ã®·àò·àµ·â≥·ãà·âµ  I-PRODUCT\n","2     ·åé·ãµ·åì·ã≥  I-PRODUCT\n","3     ·à≥·àÖ·äñ·âΩ  I-PRODUCT\n","4        ·ä®  I-PRODUCT\n","Dataset({\n","    features: ['Token', 'Label'],\n","    num_rows: 8075\n","})\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","\n","# Step 1: Load the data\n","df = pd.read_csv(\"/content/drive/My Drive/Kifiya/week 5/final_tokens_labels.csv\")\n","\n","# Display the first few rows to understand the structure\n","print(\"DataFrame head:\")\n","print(df.head())\n","\n","# Step 2: Process the 'Token' and 'Label' columns\n","df['Token'] = df['Token'].apply(lambda x: x.split())  # Split tokens\n","df['Label'] = df['Label'].apply(lambda x: x.split())  # Split labels\n","\n","# Check for empty or missing values\n","print(\"\\nChecking for missing values:\")\n","print(df.isnull().sum())\n","\n","# Step 3: Create a Hugging Face Dataset\n","dataset = Dataset.from_pandas(df)\n","\n","# Check the dataset structure\n","print(\"\\nDataset structure:\")\n","print(dataset)\n","\n","# Display the first few entries in the dataset to confirm\n","print(\"\\nDataset samples:\")\n","print(dataset[:5])  # Show first 5 entries\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLjcshPB9Iak","executionInfo":{"status":"ok","timestamp":1727784065955,"user_tz":-180,"elapsed":565,"user":{"displayName":"Yonas Kumelachew","userId":"02986940558465790424"}},"outputId":"accb7ffc-564f-4dc7-845e-7ad6795ab4d9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame head:\n","     Token      Label\n","0  ·â£·àà·â•·ãô·å•·âÖ·àù  I-PRODUCT\n","1   ·ã®·àò·àµ·â≥·ãà·âµ  I-PRODUCT\n","2     ·åé·ãµ·åì·ã≥  I-PRODUCT\n","3     ·à≥·àÖ·äñ·âΩ  I-PRODUCT\n","4        ·ä®  I-PRODUCT\n","\n","Checking for missing values:\n","Token    0\n","Label    0\n","dtype: int64\n","\n","Dataset structure:\n","Dataset({\n","    features: ['Token', 'Label'],\n","    num_rows: 8075\n","})\n","\n","Dataset samples:\n","{'Token': [['·â£·àà·â•·ãô·å•·âÖ·àù'], ['·ã®·àò·àµ·â≥·ãà·âµ'], ['·åé·ãµ·åì·ã≥'], ['·à≥·àÖ·äñ·âΩ'], ['·ä®']], 'Label': [['I-PRODUCT'], ['I-PRODUCT'], ['I-PRODUCT'], ['I-PRODUCT'], ['I-PRODUCT']]}\n"]}]},{"cell_type":"code","source":["\n","# # Step 4: Define the tokenization function\n","# from transformers import AutoTokenizer\n","\n","# # Load the tokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(\"Davlan/afro-xlmr-base\")\n","\n","# def tokenize_and_align_labels(examples):\n","#     tokenized_inputs = tokenizer(examples['Token'], truncation=True, is_split_into_words=True)\n","\n","#     labels = []\n","#     for i, label_list in enumerate(examples['Label']):\n","#         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get the word IDs for the tokens\n","#         label_ids = []\n","#         previous_word_idx = None\n","\n","#         # Aligning labels with tokens\n","#         for word_idx in word_ids:\n","#             if word_idx is None:\n","#                 label_ids.append(-100)  # Special tokens\n","#             elif word_idx != previous_word_idx:\n","#                 label_ids.append(label2id[label_list[word_idx]])  # Get label ID for the word\n","#             else:\n","#                 label_ids.append(-100)  # Subword tokens\n","#             previous_word_idx = word_idx\n","\n","#         labels.append(label_ids)\n","\n","#     tokenized_inputs[\"labels\"] = labels\n","#     return tokenized_inputs\n","\n","# # Step 5: Map the tokenization function to the dataset\n","# tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n","\n","# # Display the tokenized dataset\n","# print(tokenized_dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["c165d74fc80040aebf91a607919e06ce","961a1c526c2e4899a60a7b1e9ed3b453","7d5f0a749f794854b47c9552e7ca6c9c","2e884283b3fb48fb8f4dbdb3aea709f1","c9fd7ae3624b4c4e88083c2c7e7973e4","a5bea9323d4a4e2a865761f3a727edd4","4752e8b753b74bb4bd0232e96772da25","071932f7ee6143789e724b2b618d386f","303574550d3b450297528aaa4651a6be","6e048373ced1447f8e0e0809b2eedb50","c87106495311442c9a95b4d23f990695"]},"id":"B_rxvCntyDSH","executionInfo":{"status":"ok","timestamp":1727781321940,"user_tz":-180,"elapsed":5430,"user":{"displayName":"Yonas Kumelachew","userId":"02986940558465790424"}},"outputId":"078bf863-6106-4a9c-8ce6-c0346be8cbb5"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8075 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c165d74fc80040aebf91a607919e06ce"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['Token', 'Label', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 8075\n","})\n"]}]},{"cell_type":"code","source":["# from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n","\n","# # Step 1: Set up the training arguments\n","# training_args = TrainingArguments(\n","#     output_dir=\"./results\",                     # Output directory\n","#     evaluation_strategy=\"epoch\",                 # Evaluation strategy to adopt during training\n","#     learning_rate=2e-5,                          # Learning rate\n","#     per_device_train_batch_size=16,              # Batch size for training\n","#     per_device_eval_batch_size=16,               # Batch size for evaluation\n","#     num_train_epochs=3,                          # Total number of training epochs\n","#     weight_decay=0.01,                           # Strength of weight decay\n","#     logging_dir='./logs',                        # Directory for storing logs\n","#     logging_steps=10,\n","# )\n","\n","# # Step 2: Create a data collator\n","# data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","# # Step 3: Initialize the Trainer\n","# trainer = Trainer(\n","#     model=model,                                 # The instantiated ü§ó Transformers model to be trained\n","#     args=training_args,                          # Training arguments, defined above\n","#     train_dataset=tokenized_dataset,             # The training dataset\n","#     eval_dataset=None,                           # Optionally, specify an evaluation dataset\n","#     data_collator=data_collator,                 # Data collator\n","# )\n","\n","# # Step 4: Train the model\n","# trainer.train()\n","\n","\n"],"metadata":{"id":"KmFlC2pMzT4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","# Step 1: Load the data\n","df = pd.read_csv(\"/content/drive/My Drive/Kifiya/week 5/final_tokens_labels.csv\")\n","\n","# Display the first few rows to understand the structure\n","print(\"DataFrame head:\")\n","print(df.head())\n","\n","# Step 2: Process the 'Token' and 'Label' columns\n","df['Token'] = df['Token'].apply(lambda x: x.split())  # Split tokens\n","df['Label'] = df['Label'].apply(lambda x: x.split())  # Split labels\n","\n","# Check for empty or missing values\n","print(\"\\nChecking for missing values:\")\n","print(df.isnull().sum())\n","\n","# Step 3: Create a Hugging Face Dataset\n","dataset = Dataset.from_pandas(df)\n","\n","# Step 4: Split the dataset into training and validation sets\n","dataset = dataset.train_test_split(test_size=0.2)  # Split into training and validation\n","\n","# Load the model and tokenizer\n","model_name = \"Davlan/afro-xlmr-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Define your label to ID mapping\n","label2id = {\n","    \"B-PRODUCT\": 0,\n","    \"I-PRODUCT\": 1,\n","    \"B-PRICE\": 2,\n","    \"I-PRICE\": 3,\n","    \"B-LOC\": 4,\n","    \"I-LOC\": 5,\n","    \"O\": 6,  # 'O' for outside any named entity\n","}\n","\n","id2label = {v: k for k, v in label2id.items()}  # Create reverse mapping\n","\n","# Load the model with the number of labels defined\n","model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2id))\n","\n","# Step 5: Define the tokenization function\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples['Token'], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label_list in enumerate(examples['Label']):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get the word IDs for the tokens\n","        label_ids = []\n","        previous_word_idx = None\n","\n","        # Aligning labels with tokens\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                label_ids.append(-100)  # Special tokens\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label2id[label_list[word_idx]])  # Get label ID for the word\n","            else:\n","                label_ids.append(-100)  # Subword tokens\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs\n","\n","# Step 6: Map the tokenization function to the dataset\n","tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n","\n","# Step 7: Set up the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",                     # Output directory\n","    evaluation_strategy=\"epoch\",                 # Updated evaluation strategy\n","    learning_rate=2e-5,                          # Learning rate\n","    per_device_train_batch_size=16,              # Batch size for training\n","    per_device_eval_batch_size=16,               # Batch size for evaluation\n","    num_train_epochs=3,                          # Total number of training epochs\n","    weight_decay=0.01,                           # Strength of weight decay\n","    logging_dir='./logs',                        # Directory for storing logs\n","    logging_steps=10,\n",")\n","\n","# Step 8: Create a data collator\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","# Step 9: Initialize the Trainer\n","trainer = Trainer(\n","    model=model,                                 # The instantiated ü§ó Transformers model to be trained\n","    args=training_args,                          # Training arguments, defined above\n","    train_dataset=tokenized_dataset['train'],    # Specify the training dataset\n","    eval_dataset=tokenized_dataset['test'],      # Specify the evaluation dataset\n","    data_collator=data_collator,                 # Data collator\n",")\n","\n","# Step 10: Train the model\n","trainer.train()\n","\n","# Step 11: Evaluate the model\n","eval_results = trainer.evaluate()\n","\n","# Step 12: Save the model\n","trainer.save_model(\"./fine-tuned-model\")  # Save the model to the specified directory\n","\n","# Print evaluation results\n","print(\"Evaluation results:\", eval_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535,"referenced_widgets":["e784b89ff98f41ab9860f3a3efe3b556","d846ac993f9e4513ace7d539e4460676","15024f288f8043fcb19824502bdebe39","cf729487f6424863a00e1c8f58ca7a68","139bc3967a7b4b13903130291d27ea01","ca4ed8a848d24359b23d28d83d952b34","4e47040475614f4daf9f016457a92c20","62a3dd6f7434401ab40c0737075649bc","4c35d63ef6b347649e854e75a86e4cfc","15cd7520006f483cb0007d2d7f7ab9c4","d6c08c84533a4669b7441091c648b447","1d1bbe57d9b74ca5800d2def6a06758b","367be2cf21a14684bb4c2c8c2b11036e","3fa9e5ede949494690a50c83c7088470","f67a0318de924f8e8cbe0e1c771cc833","dc2a08c707a0425eb78481e2e2f22b79","171a3dc5c72a42a1bc1fb717e152eaea","d1e70c3bc0ff45d9b5dc2a4f16db45c1","4f3702cac459479c8f0cbfea2b1ba40b","b44b4482ca3e4fc7a4380f49de828b41","20d6c005f38747f498411526f19c66b4","095f184e73644fa5bb34b36d70aee69a"]},"id":"zcbZVA805afV","outputId":"f0116409-bc24-493a-8a37-4d01ec2f41bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame head:\n","     Token      Label\n","0  ·â£·àà·â•·ãô·å•·âÖ·àù  I-PRODUCT\n","1   ·ã®·àò·àµ·â≥·ãà·âµ  I-PRODUCT\n","2     ·åé·ãµ·åì·ã≥  I-PRODUCT\n","3     ·à≥·àÖ·äñ·âΩ  I-PRODUCT\n","4        ·ä®  I-PRODUCT\n","\n","Checking for missing values:\n","Token    0\n","Label    0\n","dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/6460 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e784b89ff98f41ab9860f3a3efe3b556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1615 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d1bbe57d9b74ca5800d2def6a06758b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1001' max='1212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1001/1212 1:36:06 < 20:17, 0.17 it/s, Epoch 2.48/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.159600</td>\n","      <td>0.251704</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.094800</td>\n","      <td>0.175507</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# # Step 5: Evaluate the model (optional)\n","# # If you have a validation dataset, you can evaluate it:\n","# # results = trainer.evaluate()\n","\n","# # Step 6: Save the model\n","# model.save_pretrained(\"./fine-tuned-model\")\n","# tokenizer.save_pretrained(\"./fine-tuned-model\")"],"metadata":{"id":"zuml84JqzbUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M9iWnVvBEMBv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install shap lime"],"metadata":{"id":"5sIP71qQD3Zp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","\n","# Load your model and tokenizer\n","model_name = \"Davlan/afro-xlmr-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForTokenClassification.from_pretrained(model_name)\n","\n","# Set model to evaluation mode\n","model.eval()\n"],"metadata":{"id":"oaKpyJTH_Vb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lime.lime_text import LimeTextExplainer\n","\n","# Define the function to make predictions\n","def predict_fn(texts):\n","    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=2)\n","    return predictions.numpy()\n","\n","# Initialize LIME\n","explainer = LimeTextExplainer(class_names=label2id.keys())\n","\n","# Example text for explanation\n","example_text = \"Your example text goes here.\"\n","\n","# Generate LIME explanation\n","exp = explainer.explain_instance(example_text, predict_fn, num_features=10)\n","\n","# Show the explanation\n","exp.show_in_notebook(text=example_text)\n"],"metadata":{"id":"sPKc9ElOD_bT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","\n","# Function to create SHAP explainer\n","def create_shap_explainer(model, tokenizer):\n","    def f(x):\n","        inputs = tokenizer(x.tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","        return torch.softmax(outputs.logits, dim=-1).numpy()\n","\n","    return shap.KernelExplainer(f, shap.sample(inputs, 100))\n","\n","# Generate SHAP values for your dataset\n","explainer = create_shap_explainer(model, tokenizer)\n","shap_values = explainer.shap_values(example_text)\n","\n","# Plot SHAP values\n","shap.initjs()\n","shap.force_plot(explainer.expected_value, shap_values[0], example_text)\n"],"metadata":{"id":"OZE6nimsEG8Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"dxWGPJNaW-Nr"}},{"cell_type":"markdown","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZUMQoMrvW4rs"}}]}